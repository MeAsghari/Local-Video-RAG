{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27822,"status":"ok","timestamp":1766584081174,"user":{"displayName":"Ù…Ù‡Ø¯ÛŒ Ø§ØµØºØ±ÛŒ","userId":"03432582110641798601"},"user_tz":480},"id":"of7tDgD4wocq","outputId":"25c2ae5c-1240-489c-ecf8-7626b655a075"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[33mWARNING: optimum 2.1.0 does not provide the extra 'exporters'\u001b[0m\u001b[33m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m56.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m161.2/161.2 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m18.1/18.1 MB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m194.2/194.2 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["!pip install optimum[exporters,onnxruntime] transformers onnxruntime -q"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":220,"status":"ok","timestamp":1766584081408,"user":{"displayName":"Ù…Ù‡Ø¯ÛŒ Ø§ØµØºØ±ÛŒ","userId":"03432582110641798601"},"user_tz":480},"id":"pVKA2eJm9FG_","outputId":"26c837d4-95f2-4ad8-e98a-7a0cf067bfc5"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/Local-Video-RAG\n"]}],"source":["cd /content/drive/MyDrive/Local-Video-RAG"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":783},"id":"l7_DkiT7wy64"},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n","Multiple distributions found for package optimum. Picked distribution: optimum\n","/usr/local/lib/python3.12/dist-packages/torch/amp/autocast_mode.py:270: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n","  warnings.warn(\n","Flax classes are deprecated and will be removed in Diffusers v1.0.0. We recommend migrating to PyTorch classes or pinning your version of Diffusers.\n","Flax classes are deprecated and will be removed in Diffusers v1.0.0. We recommend migrating to PyTorch classes or pinning your version of Diffusers.\n"]},{"name":"stdout","output_type":"stream","text":["--- 1. Exporting Float32 Model (Safe Mode) ---\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n","Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n","You are not authenticated with the Hugging Face Hub in this notebook.\n","If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bfc821242b0741bda3dae3317689f520","version_major":2,"version_minor":0},"text/plain":["config.json: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["`torch_dtype` is deprecated! Use `dtype` instead!\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fd01f0dc10d94dd58f0d731df43e7d03","version_major":2,"version_minor":0},"text/plain":["pytorch_model.bin:   0%|          | 0.00/605M [00:00\u003c?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"339c2d56994e4f89b470406393047749","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/605M [00:00\u003c?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"17da416d95ba45e9816c034cdaee6e08","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/592 [00:00\u003c?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"65ff7844035a4849912e7987fed160ab","version_major":2,"version_minor":0},"text/plain":["vocab.json: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5c38e47c8c2d4a089db07e94c78bb267","version_major":2,"version_minor":0},"text/plain":["merges.txt: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d27fad3e528b4aa687b79dc18cbd72c4","version_major":2,"version_minor":0},"text/plain":["tokenizer.json: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"62a2be98f8534f0f9d8da79b9a186442","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/389 [00:00\u003c?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"feddb712697d4f15bd36a1f88e78e5df","version_major":2,"version_minor":0},"text/plain":["preprocessor_config.json:   0%|          | 0.00/316 [00:00\u003c?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n","/usr/local/lib/python3.12/dist-packages/transformers/models/clip/feature_extraction_clip.py:30: FutureWarning: The class CLIPFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use CLIPImageProcessor instead.\n","  warnings.warn(\n","Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n","/usr/local/lib/python3.12/dist-packages/transformers/models/clip/modeling_clip.py:197: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n","  if not interpolate_pos_encoding and (height != self.image_size or width != self.image_size):\n","/usr/local/lib/python3.12/dist-packages/transformers/models/clip/modeling_clip.py:207: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n","  if interpolate_pos_encoding:\n","/usr/local/lib/python3.12/dist-packages/transformers/models/clip/modeling_clip.py:236: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n","  if seq_length \u003e max_position_embedding:\n","/usr/local/lib/python3.12/dist-packages/transformers/modeling_attn_mask_utils.py:94: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n","  if input_shape[-1] \u003e 1 or self.sliding_window is not None:\n","/usr/local/lib/python3.12/dist-packages/transformers/modeling_attn_mask_utils.py:170: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n","  if past_key_values_length \u003e 0:\n","/usr/local/lib/python3.12/dist-packages/transformers/modeling_attn_mask_utils.py:196: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n","  inverted_mask = torch.tensor(1.0, dtype=dtype) - expanded_mask\n","/usr/local/lib/python3.12/dist-packages/torch/onnx/_internal/torchscript_exporter/symbolic_opset9.py:5353: UserWarning: Exporting aten::index operator of advanced indexing in opset 18 is achieved by combination of multiple ONNX operators, including Reshape, Transpose, Concat, and Gather. If indices include negative values, the exported graph will produce incorrect results.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Float32 Export complete.\n","--- 2. Manual Dynamic Quantization ---\n"]},{"name":"stderr","output_type":"stream","text":["WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"]},{"name":"stdout","output_type":"stream","text":["Quantization complete!\n","Original Size: 577.48 MB\n","Quantized Size: 145.61 MB\n"]}],"source":["from optimum.onnxruntime import ORTModelForFeatureExtraction\n","from transformers import AutoTokenizer, AutoProcessor\n","from onnxruntime.quantization import quantize_dynamic, QuantType\n","import os\n","import shutil\n","\n","model_id = \"openai/clip-vit-base-patch32\"\n","base_dir = \"onnx_clip_model\"\n","float32_dir = os.path.join(base_dir, \"float32\")\n","quantized_dir = os.path.join(base_dir, \"quantized\")\n","\n","\n","os.makedirs(float32_dir, exist_ok=True)\n","os.makedirs(quantized_dir, exist_ok=True)\n","\n","print(f\"--- 1. Exporting Float32 Model (Safe Mode) ---\")\n","\n","model = ORTModelForFeatureExtraction.from_pretrained(model_id, export=True)\n","tokenizer = AutoTokenizer.from_pretrained(model_id)\n","processor = AutoProcessor.from_pretrained(model_id)\n","\n","\n","model.save_pretrained(float32_dir)\n","tokenizer.save_pretrained(quantized_dir)\n","processor.save_pretrained(quantized_dir)\n","print(\"Float32 Export complete.\")\n","\n","print(\"--- 2. Manual Dynamic Quantization ---\")\n","\n","\n","\n","input_model_path = os.path.join(float32_dir, \"model.onnx\")\n","\n","output_model_path = os.path.join(quantized_dir, \"model_quantized.onnx\")\n","\n","quantize_dynamic(\n","    model_input=input_model_path,\n","    model_output=output_model_path,\n","    weight_type=QuantType.QUInt8\n",")\n","\n","print(f\"Quantization complete!\")\n","print(f\"Original Size: {os.path.getsize(input_model_path) / (1024*1024):.2f} MB\")\n","print(f\"Quantized Size: {os.path.getsize(output_model_path) / (1024*1024):.2f} MB\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cS6-aVMp281y"},"outputs":[],"source":["import shutil\n","from google.colab import files\n","\n","\n","shutil.make_archive('my_edge_ai_model', 'zip', 'onnx_clip_model')\n","\n","files.download('my_edge_ai_model.zip')"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1885,"status":"ok","timestamp":1766567175819,"user":{"displayName":"Ù…Ù‡Ø¯ÛŒ Ø§ØµØºØ±ÛŒ","userId":"03432582110641798601"},"user_tz":480},"id":"8Wa65hUDAxUq","outputId":"45230b40-81fe-4487-dce6-5adc28e51730"},"outputs":[{"name":"stdout","output_type":"stream","text":["--- 1. Loading ONNX Model via Raw ONNX Runtime ---\n","âœ… Model loaded successfully into ONNX Runtime!\n","--- 2. Pre-processing Inputs ---\n","--- 3. Running Inference ---\n","\n","--- âœ… Final Results ---\n","Confidence for 'a photo of a cat': 98.42%\n","Confidence for 'a photo of a dog': 1.19%\n","Confidence for 'a car': 0.39%\n","\n","--- ğŸ” Model Signature (Save this for C++) ---\n","Inputs:\n","  - Name: input_ids, Shape: ['text_batch_size', 'sequence_length'], Type: tensor(int64)\n","  - Name: pixel_values, Shape: ['batch_size', 'num_channels', 'height', 'width'], Type: tensor(float)\n","  - Name: attention_mask, Shape: ['text_batch_size', 'sequence_length'], Type: tensor(int64)\n","Outputs:\n","  - Name: logits_per_image, Shape: ['image_batch_size', 'text_batch_size'], Type: tensor(float)\n","  - Name: logits_per_text, Shape: ['text_batch_size', 'image_batch_size'], Type: tensor(float)\n","  - Name: text_embeds, Shape: ['text_batch_size', 512], Type: tensor(float)\n","  - Name: image_embeds, Shape: ['image_batch_size', 512], Type: tensor(float)\n"]}],"source":["import onnxruntime as ort\n","from transformers import AutoProcessor\n","from PIL import Image\n","import requests\n","import numpy as np\n","import os\n","\n","\n","quantized_model_path = \"onnx_clip_model/quantized/model_quantized.onnx\"\n","config_path = \"onnx_clip_model/quantized\"\n","\n","print(\"--- 1. Loading ONNX Model via Raw ONNX Runtime ---\")\n","try:\n","\n","    session = ort.InferenceSession(quantized_model_path)\n","    print(\"âœ… Model loaded successfully into ONNX Runtime!\")\n","except Exception as e:\n","    print(f\"âŒ Error loading model: {e}\")\n","    exit()\n","\n","print(\"--- 2. Pre-processing Inputs ---\")\n","processor = AutoProcessor.from_pretrained(config_path)\n","\n","\n","url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n","image = Image.open(requests.get(url, stream=True).raw)\n","texts = [\"a photo of a cat\", \"a photo of a dog\", \"a car\"]\n","\n","\n","inputs = processor(text=texts, images=image, return_tensors=\"np\", padding=True)\n","\n","\n","ort_inputs = {\n","    \"input_ids\": inputs[\"input_ids\"].astype(np.int64),\n","    \"pixel_values\": inputs[\"pixel_values\"].astype(np.float32),\n","    \"attention_mask\": inputs[\"attention_mask\"].astype(np.int64)\n","}\n","\n","print(\"--- 3. Running Inference ---\")\n","\n","outputs = session.run(None, ort_inputs)\n","\n","\n","logits_per_image = outputs[0]\n","probs = (np.exp(logits_per_image) / np.sum(np.exp(logits_per_image), axis=1, keepdims=True))\n","\n","print(\"\\n--- âœ… Final Results ---\")\n","for i, text in enumerate(texts):\n","    print(f\"Confidence for '{text}': {probs[0][i]*100:.2f}%\")\n","\n","\n","print(\"\\n--- ğŸ” Model Signature (Save this for C++) ---\")\n","print(\"Inputs:\")\n","for i in session.get_inputs():\n","    print(f\"  - Name: {i.name}, Shape: {i.shape}, Type: {i.type}\")\n","\n","print(\"Outputs:\")\n","for i in session.get_outputs():\n","    print(f\"  - Name: {i.name}, Shape: {i.shape}, Type: {i.type}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8viCwcWmLezT"},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOf48ZViukUDQtv7qIPx29W","mount_file_id":"1sV6C2rp2o03-Zzzey5GEXkFc8qBkZ3Tr","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{}}},"nbformat":4,"nbformat_minor":0}